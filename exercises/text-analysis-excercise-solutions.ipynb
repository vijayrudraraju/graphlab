{
 "metadata": {
  "name": "",
  "signature": "sha256:ee7ae00b9641ec761af1f897192918b9dcbe8c095181f1e08d086ebb84f2cefb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Analyzing text with GraphLab Create\n",
      "\n",
      "The data for this exercise is culled from [Wikipedia's Database Download](http://en.wikipedia.org/wiki/Wikipedia:Database_download). Wikipedia's text and many of its images are co-licensed under the [Creative Commons Attribution-Sharealike 3.0 Unported License (CC-BY-SA)](http://creativecommons.org/licenses/by-sa/2.5/)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import graphlab\n",
      "graphlab.canvas.set_target(\"ipynb\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load the first Wikipedia text file called \"w0\". Each line in the document represents a single document and there is no header line. Name the variable `documents`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "documents = graphlab.SFrame.read_csv('wikipedia/w1', header=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:red\">**Question 1:**</span> Create an [SArray](http://graphlab.com/products/create/docs/generated/graphlab.SArray.html#graphlab.SArray) that represents the documents in \"bag-of-words format\", where each element of the SArray is a dictionary with each unique word as a key and the number of occurrences is the value. *Hint*: look at the SArray method [count_words](http://graphlab.com/products/create/docs/generated/graphlab.SArray.count_words.html)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bow = documents['X1'].count_words()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:red\">**Question 2:**</span> Create a trimmed version of this dataset that excludes all words in each document that occur just once."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docs = bow.dict_trim_by_values(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:red\">**Question 3:**</span> Remove all stopwords from the dataset. *Hint*: you'll find a predefined set of stopwords in [graphlab.text.stopwords](http://graphlab.com/products/create/docs/generated/graphlab.text.util.stopwords.html#graphlab.text.util.stopwords)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docs = docs.dict_trim_by_keys(graphlab.text.stopwords(), exclude=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:red\">**Question 4:**</span> Remove all documents from `docs` and `documents` that now have fewer than 10 unique words. *Hint*: You can use SArray's [logical filter](http://graphlab.com/products/create/docs/generated/graphlab.SArray.__getitem__.html)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "ix = docs.apply(lambda x: len(x.keys()) >= 10)\n",
      "docs = docs[ix]\n",
      "documents = documents[ix]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:red\">**Question 5:**</span> What proportion of documents have we removed from the dataset?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "1 - ix.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Topic Modeling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:red\">**Question 6:**</span> Create a topic model using your processed version of the dataset, `docs`. Have the model learn 30 topics and let the algorithm run for 30 iterations. *Hint*: use the new [topic modeling toolkit](http://graphlab.com/products/create/docs/generated/graphlab.text.topic_model.TopicModel.html#graphlab.text.topic_model.TopicModel)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m = graphlab.topic_model.create(docs, num_topics=30, num_iterations=30)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:red\">**Question 7:**</span> Print information about the model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:red\">**Question 8:**</span> Find out how many words the model has used while learning the topic model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(m['vocabulary'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use the following code to get the top 10 most probable words in each topic. Typically we hope that each list is a cohesive set of words, one that represents a general cluster of topics present in the dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topics = m.get_topics(num_words=10).unstack(['word','score'], new_column_name='topic_words')['topic_words'].apply(lambda x: x.keys())\n",
      "for topic in topics:\n",
      "    print topic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:red\">**Question 9:**</span> Predict the topic for the first 5 documents in `docs`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m.predict(docs[:5])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sometimes it is useful to manually fix words to be associated with a particular topic. For this we can use the `associations` argument.\n",
      "\n",
      "<span style=\"color:red\">**Question 10:**</span> Create a new topic model that uses the following SFrame which will associate the words \"law\", \"court\", and \"business\" to topic 0. Use `verbose=False`, 30 topics, and let the algorithm run for 20 iterations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fixed_associations = graphlab.SFrame()\n",
      "fixed_associations['word'] = ['law', 'court', 'business']\n",
      "fixed_associations['topic'] = 0\n",
      "m2 = graphlab.topic_model.create(docs,  \n",
      "                                 associations=fixed_associations,\n",
      "                                 num_topics=30, verbose=False, num_iterations=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:red\">**Question 11:**</span> Get the top 20 most likely words for topic 0. Ideally, we will see the words \"law\", \"court\", and \"business\". What other words appear to be related to this topic?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m2.get_topics([0], num_words=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Transforming word counts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remove all the documents from `docs` and `documents` that have 0 words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tf_idf_docs = graphlab.toolkits.text.tf_idf(docs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:red\">**Question 12:**</span> Use GraphLab Canvas to explore the distribution of TF-IDF scores given to the word \"year\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tf_idf_docs.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:red\">**Question 13:**</span> Create an SFrame with the following columns:\n",
      "    \n",
      "- `id`: a string column containing the range of numbers from 0 to the number of documents\n",
      "- `word_score`: the SArray containing TF-IDF scores you created above\n",
      "- `text`: the original text from each document"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc_data = graphlab.SFrame()\n",
      "doc_data['id'] = graphlab.SArray(range(len(tf_idf_docs))).astype(str)\n",
      "doc_data['word_score'] = tf_idf_docs\n",
      "doc_data['text'] = documents['X1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:red\">**Question 14:**</span> Create a model that allows you to query the nearest neighbors to a given document. Use the `id` column above as your label for each document, and use the `word_score` column of TF-IDF scores as your features. *Hint*: use the new [nearest_neighbors toolkit](http://graphlab.com/products/create/docs/generated/graphlab.toolkits.nearest_neighbors.html)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = graphlab.nearest_neighbors.create(doc_data, label='id', features=['word_score'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:red\">**Question 15:**</span> Find all the nearest documents for the first two documents in the data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nearest = nn.query(doc_data.head(2), label='id', features=['word_score'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:red\">**Question 16:**</span> Make an SFrame that contains the original text for the query points and the original text for each query's nearest neighbors. *Hint*: Use [SFrame.join](http://graphlab.com/products/create/docs/generated/graphlab.SFrame.join.html#graphlab.SFrame.join)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nearest_docs = nearest[['query_label', 'reference_label']]\n",
      "doc_data = doc_data[['id', 'text']]\n",
      "nearest_docs.join(doc_data, on={'query_label':'id'})\\\n",
      "            .rename({'text':'query_text'})\\\n",
      "            .join(doc_data, on={'reference_label':'id'})\\\n",
      "            .rename({'text':'original_text'})\\\n",
      "            .sort('query_label')[['query_text', 'original_text']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Counting words in Wikipedia using GraphLab Jobs & Tasks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "GraphLab has a [data pipeline framework](http://graphlab.com/products/create/docs/generated/graphlab.SFrame.deploy.html) that can facilitate deploying data products to EC2 or Hadoop where your complete data sets may reside. Let's revisit some of the functionality above to demonstrate how you might compute word counts for all words in the corpus Wikipedia articles. The first thing we'll do is define a Task for reading in the input data and getting word counts for each document."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from graphlab.deploy import Task\n",
      "\n",
      "def ingest(task):\n",
      "    raw_data_path = task.params[\"raw_data_path\"]\n",
      "    wiki_sf = graphlab.SFrame.read_csv(raw_data_path, header=False)\n",
      "    wiki_sf[\"word_counts\"] = wiki_sf['X1'].count_words()\n",
      "    task.outputs[\"wiki_sf\"] = wiki_sf\n",
      "\n",
      "ingest_task = Task(\"ingest\")\n",
      "ingest_task.set_code(ingest)\n",
      "ingest_task.set_params([\"raw_data_path\"])\n",
      "ingest_task.set_outputs([\"wiki_sf\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:red\">**Question 17:**</span> Now that you have a sense of the [Task API](http://graphlab.com/products/create/docs/generated/graphlab.deploy.Task.html), see if you can write yet another task that will both a) filter out stopwords and b) aggregate counts for the entire corpus. *Hint*: have a look at the flat_map and and group_by methods on [SFrame](http://graphlab.com/products/create/docs/generated/graphlab.SFrame.html)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def count_words(task):\n",
      "    wiki_sf = task.inputs[\"wiki_sf\"]\n",
      "    wiki_sf[\"wc_stop_filtered\"] = wiki_sf[\"word_counts\"].dict_trim_by_keys(\n",
      "        graphlab.text.stopwords(), exclude=True)\n",
      "    word_counts = wiki_sf.flat_map([\"word\", \"count\"], \n",
      "                                    lambda x: [list(p) for p in x['wc_stop_filtered'].iteritems()])\n",
      "    word_counts = word_counts.groupby(key_columns=[\"word\"], \n",
      "                                      operations={'total_count':graphlab.aggregate.SUM(\"count\")})\n",
      "    task.outputs[\"word_counts\"] = word_counts\n",
      "\n",
      "count_words_task = Task(\"count_words\")\n",
      "count_words_task.set_code(count_words)\n",
      "count_words_task.set_inputs([\"wiki_sf\"])\n",
      "count_words_task.set_outputs([\"word_counts\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point, we still haven't actually executed any of our Tasks. When creating data pipelines like this, it's a good idea to run locally on a subset of the data before deploying to Hadoop or EC2 to run on our complete data set."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*NOTE*: update the paths below to reflect where your dataset is stored. If there is an error, try using absolute paths."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "graphlab.deploy.environments.delete(\"release-local-async\")\n",
      "\n",
      "tasks = [\n",
      "    (ingest_task, {\"raw_data_path\": \"/home/brian/graphlab-projects/graphlab-exercises/wikipedia/w1*\"}),\n",
      "    (count_words_task, {\n",
      "        \"wiki_sf\": (ingest_task, \"wiki_sf\"),\n",
      "        \"word_counts\": \"wikipedia/word_counts.sframe\"})]\n",
      "\n",
      "local_env = graphlab.deploy.environment.LocalAsync(\"release-local-async\")\n",
      "job = graphlab.deploy.job.create(tasks, environment=local_env)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calling [graphlab.deploy.job.create()](http://graphlab.com/products/create/docs/generated/graphlab.SFrame.deploy.html#graphlab.deploy.job.create) has the effect of submitting a job. Because we chose LocalAsync as our [Environment](http://graphlab.com/products/create/docs/generated/graphlab.SFrame.deploy.html#graphlab.deploy.Environment), the Task is executed in the background. We can check on our job's status by calling the [get_status method](http://graphlab.com/products/create/docs/generated/graphlab.SFrame.deploy.html#graphlab.deploy.Job.get_status) on our Job. In order to submit this job to a remote cluster, we would have to change our environment to Hadoop or EC2, and make sure we pass along the appropriate paramters (eg. AWS credentials and remote file paths). This is left as an exercise to the reader."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:red\">**Question 16:**</span> Open the word_counts SFrame generated above and sort by the count column. What are the ten most frequently occurring terms in Wikipedia?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_counts = graphlab.SFrame(\"wikipedia/word_counts.sframe\")\n",
      "word_counts.sort([\"total_count\"], ascending=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the examples above, we defined 2 Tasks to ingest all of Wikipedia, count the words in each document, filter stopwords, and generate aggregated counts for all of Wikipedia. We could have broken down this pipeline into more Tasks or consolidated them into a single Task. The right choice depends on the use case, but it's often useful to preserve the output of our data transformations for posterity. Additionally, writing smaller functions makes your code more modular, more readable, and easier to debug.\n",
      "\n",
      "Take a moment and try to come up with some additional questions you can ask of this data. For example, how many tokens are there in the corpus? How many tokens are there in the corpus after stopwords have been removed? How many unique terms are there in the corpus? What other transformations can you think of that might provide an interesting view of the corpus? Have fun!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}